\documentclass{beamer}
\usetheme{Warsaw}

\usepackage{color}
\usepackage{CJK}
\usepackage{listings}
\usepackage{url}
\usepackage{booktabs}
\usepackage{pgf}
\usepackage{multicol}

\input{slide-macro}

\begin{document}
\begin{CJK}{UTF8}{bsmi}

\title{Parallel Computer Architecture}

\author{Pangfeng Liu \\ National Taiwan University}

\begin{frame}
\titlepage
\end{frame}

% \section{Introduction} 

\begin{frame}
\frametitle{Architecture}
\begin{itemize}
\item Multiprocessor
\item Multicomputer
\item Flynn's Taxonomy
\end{itemize}
\end{frame}

\section{Multiprocessor}

\begin{frame}
\frametitle{Uniprocessor}
\centerline{\pgfimage[height=0.4\textheight]{single-processor.pdf}}
\begin{itemize}
\item This is how we think of a computer.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Uniprocessor}
\begin{itemize}
\item A single processor for computation.
\item A single memory for storing instructions and data.
\item The CPU fetches instructions from the memory, executes the
  instructions, updates the contents of registers, and possibly data in
  the memory, and then repeats.
\item Intel 486, Pentium, etc.
\end{itemize}
\end{frame}


\subsection{Multiple Uniprocessors}

\begin{frame}
\frametitle{Multiple Uniprocessors}
\begin{itemize}
\item It is intuitive to have multiple uniprocessors working together
  to have high performance.
\item Why? It is {\em expected} that having more processors to work
  together, we can solve the problem faster.
\item How do they work {\em together}?
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Work Together?}
\centerline{\pgfimage[width=\textwidth]{multiple-single-processor}}
\end{frame}

\begin{frame}
\frametitle{Communication} 
\huge Processors must communicate to
coordinate their actions, and exchange data if necessary.
\end{frame}


\begin{frame}
\frametitle{Work Together?}
\centerline{\pgfimage[width=\textwidth]{multiprocessor-single-memory}}
\begin{itemize}
\item We cannot simply connect all processors to a memory.
\item The memory will be in an {\em inconsistent} state.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Race Condition}
\begin{itemize}
\item Suppose two processors want to add 1 to the same counter.
\item The first processor fetches the old contents of the counter, adds
  1 to it.
\item Before the first processor can store the new content, the
  second processor fetches the old content.
\item The first processor now stores its new content to memory.
\item The second processor adds 1 to the old content, and stores the
  new content back to memory.
\item The counter only increases by 1, which is {\em incorrect}.
\item More details later.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Multiple Uniprocessors}
\begin{itemize}
\item We put a {\em memory management/arbitration unit} between the
  processors and the memory so that every processor can access memory.
\item This memory management unit must be very efficient in providing
  {\em point-to-point} data transfer so as to provide fast memory
  access for every processor.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Multiprocessor}
\centerline{\pgfimage[width=\textwidth]{multiple-processor-switch}}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item What could happen if three processors want to add 1, 2, and 4
  into a memory wiht an initialized value 0?
\end{itemize}
\end{frame}

\subsection{Shared Memory}

\begin{frame}
\frametitle{Shared Memory}
\begin{itemize}
\item Logically we do not care about the memory management unit -- we
  simple believe that every processor can access this {\em shared
    memory}.
\item This shared memory provides a {\em shared addressing space} for
  all processors.
\item This {\em shared} memory is also a {\em global memory}.
\item The cost for every processor to access every part of the memory
  is the same, then we have Uniform Memory Access (UMA).
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Distributed Shared Memory}
\begin{itemize}
\item A distributed version of shared memory.
\item Every processor has a local memory, and the collection of the
  memory form a global memory.
\item We connect the processors to a memory management unit, which
  determines the address is local or remote.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Multiprocessor}
\centerline{\pgfimage[width=\textwidth]{distributed-multiprocessor}}
\end{frame}

\begin{frame}
\frametitle{Distributed Multiprocessor}
\begin{itemize}
\item If the address is local, then it is retrieved from the
  local memory.
\item If the address is remote, then it is retrieved from someone
  else's  local memory.
\item The cost for every processor to access every part of the memory
  is {\em not} the same, then we have Non-Uniform Memory Access
  (NUMA).
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Describe the difference between NUMA and UMA.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Multiprocessor}
\begin{itemize}
\item Logically we do not care about the memory management unit -- we
  simple believe that every processor can access this {\em shared
    memory}.
\item This shared memory provides a {\em shared addressing space} for
  all processors.
\item This {\em shared} memory is also a {\em global memory}.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{A Logical View}
\begin{itemize}
\item In summary, we can think of our multiprocessor is like this.
\end{itemize}
\centerline{\pgfimage[width=\textwidth]{multiprocessor}}
\end{frame}

\begin{frame}
\frametitle{Multiprocessor}
\begin{itemize}
\item Conceptually multiple processors are connected by a shared
  memory -- that is all we need to know in terms of programming.
\item However, in some programming models we still need to consider
  the possibility of race condition, and use the construct provided
  by the programming model to avoid it.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Functions} 
\begin{itemize}
\item Every processor has its own computing resource, like ALU,
  registers, etc, so you can run multiple processes on them
  simultaneously.
\item Every processor works on the tasks assigned to it, using its own
  computing resource.
\item Every processor can read and write the shared memory, so as to
  communicate or synchronize with other processors.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Synchronization}
\begin{itemize}
\item Processor can synchronize with each other by a shared memory.
\item For example, in a barrier synchronization, a processor cannot
  proceed until all others have reached the same conclusion.
\item We can set a shared variable {\tt count}.  Every finished
  processor add 1 to {\tt count}.  When the value of {\tt count}
  reaches the number of processors then every processor knows that it
  has synchronized with everyone.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Describe the dependency graph using {\em functions} and {\em
  synchronization}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Memory Conflict}
\begin{itemize}
\item Multiple processors can access the memory at the same time,
  causing conflicts.
\item When a computation has different outcome due to different
  execution order, we have a {\em race condition}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{An Example}
\begin{itemize}
\item Suppose two processors want to add their variable {\tt n}
  together into a global variable {\tt sum}.  
\item Processor 1 has {\tt n = 3} and processor 2 has {\tt n = 4}.
\item The computation consists of the following.
  \begin{itemize}
  \item Load {\tt sum} into a register {\tt r1}.
  \item Add your {\tt n} into {\tt r1}.
  \item Store register {\tt r1} back to {\tt sum}.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Race Condition}
\begin{itemize}
\item Now imagine what will happen when P1 and P2 are doing this
  simultaneously.
\item How many different outcomes could there be?  Please try to
  enumerate them.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Software Solution}
\begin{itemize}
\item Programming environment must provide mechanism that prevents
  race condition.
\item Critical section, lock, synchronization, etc.
\item More specific details will be provided when we discuss parallel
  programming.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Describe the concpet ``critical section''.
\end{itemize}
\end{frame}


\subsection{Cache}

\begin{frame}
\frametitle{Cache}
\begin{itemize}
\item To make the case even worst, we need to deal with cache.
\item Cache are fast memory -- usually they are small and expensive.
\item To improve performance we have cache for those often used
  data/instructions.  If we need the data/instructions again we can
  access them in fast cache, instead of slow main memory.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Cache for Uniprocessor}
\centerline{\pgfimage[height=0.6\textheight]{single-processor-cache}}
\end{frame}

\begin{frame}
\frametitle{Race Condition}
\begin{itemize}
\item Now every processor has a cache, and it can take data from its
  cache, not the main memory.
\item Suppose one processor changes the content of the memory, what
  should happen to the cached data in some processor's cache?
\item If one processor changes the data in its cache, would other
  processor be able to notice this change?
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Cache}
\centerline{\pgfimage[width=\textwidth]{multiprocessor-cache}}
\end{frame}

\begin{frame}
\frametitle{Hardware Solution}
\begin{itemize}
\item The hardware must guarantee that the memory and cache are in a
  consistent state.  There are various levels of guarantees.
\item If one processor changes the content of the memory, the hardware
  should invalid data that have been cached in other processor.
\item If one processor changes the data in its cache, other processor
  should be able to see it.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Writing Policy}
\begin{description}
\item[Write-through] Write is done synchronously both to the cache and
  to the backing store.
\item[Write-back] Initially, writing is done only to the cache. The
  write to the backing store is postponed until the cache blocks
  containing the data are about to be accessed by others.
\end{description}
\end{frame}


\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Describe the difference between ``Write-through'' and
  ``Write-back'' caching.
\end{itemize}
\end{frame}


\subsection{Examples}

\begin{frame}
\frametitle{Intel Gulftown CPU} 
\begin{itemize}
\item Core i7-9xx
\item 6 cores
\item 32nm
\item 12MB L3 cache
\item Introduced January 2011.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Gulftown}
\centerline{\pgfimage[width=0.9\textwidth]{gulftown_block_diagram.jpg}}
\footnote{\url{http://gizmodo.com/5491045/intels-6+core-gulftown-gets-tested-blows-us-away}}
\end{frame}

\begin{frame}
\frametitle{Gulftown}
\centerline{\pgfimage[width=0.6\textheight]{1015564707405292968.jpg}}
\footnote{\url{http://global.hkepc.com/database/images/2009/08/source/1015564707405292968.jpg}}
\end{frame}

\begin{frame}
\frametitle{Gulftown}
\begin{itemize}
\item The memory controller controls and coordinates the access to a
  shared memory.
\item Cores can communicate with the queue.  
\item Two L3 caches, and each is shared by three cores.  
  \begin{itemize}
    \item Shared L3 cache indicates that if we place wrong
      processes/threads into the cores that share the L3 cache, the
      performance will suffer.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Intel CPU's}
\begin{itemize}
\item You can find all the facts of Intel CPU here.
\item http://www.intel.com/pressroom/kits/quickreffam.htm
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Nvidia Tesla}
\begin{itemize}
\item The Tesla graphics processing unit (GPU) is Nvidia's third brand
  of GPUs. 
\item Tesla is based on high-end GPUs from the G80 (and on), as well
  as the Quadro lineup.
\item Tesla is Nvidia's first dedicated {\em General Purpose GPU}
  (GPGPU).
\item \url{http://en.wikipedia.org/wiki/Nvidia_Tesla}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{GPU Architecture}
\centerline{\pgfimage[height=0.8\textheight]{GPU-hardware}}
\end{frame}

\begin{frame}
\frametitle{Nvidia Tesla}
\centerline{\pgfimage[width=0.8\textwidth]{tesla}}
\end{frame}

\begin{frame}
\frametitle{C1060}
\begin{itemize}
\item 240 processors at 1.30 GHz.
\item 4096 MB of GDDR3 memory.
\item 102.4 GB/s memory bandwidth.
\item 933.12 GFLOPs single precision, 77.76 GFLOPs double precision.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Tesla Architecture}
\centerline{\pgfimage[width=0.8\textwidth]{tesla-architecture}}
\end{frame}

\begin{frame}
\frametitle{Characteristics}
\begin{itemize}
\item GPU is a device, and it needs a host to get its
  data/instructions.
\item A large number of cores -- usually much larger than a CPU.
\item Instruction are streamed to processors for execution, which
  means they must run the same set of instructions.
\item Processors have their local memory, as well as access to a
  shared device memory.
\item More details in the ``OpenCL'' lectures.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Describe the difference between CPU and GPU.
\end{itemize}
\end{frame}


\section{Multicomputer}

\begin{frame}
\frametitle{Multiple Uniprocessor}
\begin{itemize}
\item It is intuitive to have multiple uniprocessors working together
  to have high performance.
\item How do they work {\em together}?
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Work Together}
\centerline{\pgfimage[width=\textwidth]{multiple-single-processor}}
\end{frame}


\begin{frame}
\frametitle{Multiple Uniprocessor}
\begin{itemize}
\item We connect all the computers with a {\em network} so they can
  send {\em messages} to each other.
\item This network could be very slow or very fast, depending on the
  applications.
\item The network must provide {\em point-to-point} data transfer so
  move data from one processor to another.
\item Processors can only synchronize themselves via the network, which
  is a difficult task.
\end{itemize}
\end{frame}

\subsection{Network}

\begin{frame}
\frametitle{A Network}
\centerline{\pgfimage[width=\textwidth]{multicomputer}}
\end{frame}

\begin{frame}
\frametitle{Functions} 
\begin{itemize}
\item Every processor works on the tasks assigned to it, using its own
  computing resource.
\item Every processor can send message to each other, so as to
  communicate or synchronize with other processors.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Multicomputer}
\begin{itemize}
\item Sometimes we use the term ``node'', since these processors are
  by themselves ``computers''.
  \begin{itemize}
    \item Remember the 16,000 nodes in the Tianhe 2 cluster.
  \end{itemize}
\item Each node has its own CPU's, memory, even I/O devices.
\item The nodes can communicate with each other by the network, usually
  through standard TCP/IP protocol.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Memory}
\begin{itemize}
\item There is no {\em shared addressing space} for all processors --
  each processor use its own memory.
\item The memory of every processor is called {\em local memory}.
\item Since a memory is accessed by only one processor, we do {\em
  not} have memory conflict.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Synchronization}
\begin{itemize}
\item Processor can synchronize with each other by messages.
\item In a barrier synchronization, a processor cannot proceed until
  all others have reached the same conclusion.
\item We can ask every finished processor to send a message to a {\em
  master} when it is done.
\item When the master receives a message from every processor, he
  knows everybody has finished.
\item The master sends a message to everyone that it can continue.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Compare the way multicomputer and multiprocessot does a barrier
 synchronization.
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Network}
\begin{itemize}
\item We do care about the network -- a fast network provide better
  connectivity.\footnote{\url{http://en.wikipedia.org/wiki/Network_bandwidth}}
\begin{itemize}
\item Ethernet 10Mbit/s
\item Fast Ethernet 100Mbit/s
\item Gigabit Ethernet 1 Gbit/s
\item 10 Gigabit Ethernet 10  Gbit/s
\item Myrinet 10 Gbit/s
\item 100 Gigabit Ethernet 100 Gbit/s
\item InfiniBand (12X EDR) 300 Gbit/s
\end{itemize}

\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Topology}
\begin{itemize}
\item We cannot connect a large number of nodes to a single switch, so
  the topology of the network becomes important.
\begin{itemize}
\item Ring
\item Tree and fat tree
\item Two or higher dimensional mesh and torus
\item Hypercube
\item FFT (butterfly)
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Fat Tree}
\centerline{\pgfimage[width=0.9\textheight]{fat-tree}}
\footnote{\url{http://clusterdesign.org/wp-content/uploads/2012/02/fat_tree_varying_ports-600x365.png}}
\end{frame}

\begin{frame}
\frametitle{Mesh and Torus}
\centerline{\pgfimage[width=0.9\textheight]{mesh-torus.jpg}}
\footnote{\url{http://ars.els-cdn.com/content/image/1-s2.0-S1383762107000495-gr2.jpg}}
\end{frame}

\begin{frame}
\frametitle{FFT}
\centerline{\pgfimage[width=0.8\textheight]{FFT}}
\footnote{\url{http://cnx.org/content/m16352/latest/File0046.png}}
\end{frame}

\begin{frame}
\frametitle{Trend}
\begin{itemize}
\item The most popular topology appears to be fat tree.
\begin{itemize}
\item For example, the Tianhe 2 cluster connects all its processors
  with 13 switches as fat tree.
\end{itemize}
\item Two or higher dimension torus are also popular.
\begin{itemize}
\item The Tofu network of K-computer is a six dimensional torus.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item What is the difference between torus and a mesh?
\end{itemize}
\end{frame}

\subsection{Massively Parallel Computer}

\begin{frame}
\frametitle{Massively Parallel Computer}
\begin{itemize}
\item A computer cluster consists of a set of {\em tightly} connected
  {\em high performance} computers that work together so that in many
  respects they can be viewed as a single system.
\item Tightly connected means they are connected by {\em fast}
  network.
\item The performance is paramount, and usually achieved by
  aggregation of a large number of processors.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Top 500}
\begin{itemize}
\item The TOP500 project ranks and details the 500 most powerful
  (non-distributed) computer systems in the world.
\item The project aims to provide a reliable basis for tracking and
  detecting trends in high-performance computing and bases rankings on
  HPL, a portable implementation of the High-Performance LINPACK
  benchmark written in Fortran for distributed-memory computers.\footnote{\url{http://top500.org/}}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Top 500}
\centerline{\pgfimage[height=0.6\textheight]{Supercomputers}}
\begin{itemize}
\item Exponential growth of supercomputers performance, based on data
  from top500.org\footnote{\url{http://en.wikipedia.org/wiki/File:Supercomputers.png}}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{K computer}
\begin{itemize}
\item The K computer -- named for the Japanese word ``kei'' (äº¬),
  meaning 10 quadrillion ($10^{16}$).
\item A supercomputer manufactured by Fujitsu, currently installed at
  the RIKEN Advanced Institute for Computational Science campus in
  Kobe, Japan.
\item In June 2011, TOP500 ranked K the world's fastest supercomputer,
  with a rating of over 8 petaflops, and in November 2011, K became
  the first computer to top 10 petaflops.\footnote{\url{http://en.wikipedia.org/wiki/K_computer}}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{K Computer}
\centerline{\pgfimage[width=\textwidth]{k-computer}}
\footnote{\url{http://cdn0.sbnation.com/entry_photo_images/2197300/k-computer_large_verge_medium_landscape.jpg}}
\end{frame}

\begin{frame}
\frametitle{Configuration}
\begin{itemize}
\item 864 cabinets, 88,128 SPARC64 VIIIfx processors, over 640,000 cores.
\item A proprietary six-dimensional torus interconnect called Tofu.
\item A two-level local/global file system with parallel/distributed
  functions, which provides users with an automatic staging function for
  moving files between global and local file systems.
\item Linux operating system.
\item 9.89 MW -- the equivalent of almost 10,000 suburban homes.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{K Computer}
\centerline{\pgfimage[width=0.8\textwidth]{k-computer-rack}}
\footnote{\url{http://cdn-static.zdnet.com/i/story/30/40/093162/k-computer-riken-4.jpg}}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item What is the difference between K-computer and Tianhe 2?
\end{itemize}
\end{frame}


\subsection{Cluster}

\begin{frame}
\frametitle{Cluster}
\begin{itemize}
\item A computer cluster consists of a set of {\em loosely} connected
  computers that work together so that in many respects they can be
  viewed as a single system.
\item Loosely connected means they are {\em not} connected by fast
  network.
\item An economical alternative to those who cannot afford expensive
  parallel computers.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{TrendMicro Cluster}
\begin{itemize}
\item A cluster donated by TrendMicro for the Cloud Computing Program.
\item Loosely connected means they are not connected by fast network.
\item A economical alternative to those who cannot afford expensive
  parallel computers.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Configuration}
\begin{itemize}
\item 1 cabinet, 15 Intel X5570 quad-core processors, 120 cores.
\item A standard Ethernet switch.
\item Gluster file system.
\item Roystonea operating system, developed by Parallel and
  Distributed Processing Laboratory, Department of Computer Science
  and Information Engineering, National Taiwan University.
\item Never made it to top 500.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{TrendMicro Cluster}
\centerline{\pgfimage[height=0.8\textheight]{trend-2}}
\end{frame}

\begin{frame}
\frametitle{TrendMicro Cluster}
\centerline{\pgfimage[height=0.8\textheight]{trend-1}}
\end{frame}

\begin{frame}
\begin{itemize}
\item We (the Parallel and Distributed Processing Laboratory) learn
  many things in building Roystonea for the Trend cluster.
\item The things we learned include network virtual machine management,
  virtual machines deployment, network management for cluster,
  distributed file system for cluster, distributed database and NoSQL
  database for cloud system.
\item We built a cloud OS called ``Roystonea'' to manage the Trend
  cludter.
\item These experiences later enables us to take on more ambitious
  projects, like the optimization of billing system of ChungHwa
  Telecommunication.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item What does the name ``Roystonea'' come from?
\end{itemize}
\end{frame}

\section{Flynn's Taxonomy}

\begin{frame}
\frametitle{Flynn's Taxonomy}
\begin{itemize}
\item Single instruction stream and multiple instruction stream.
\item Single data stream and multiple data stream.
\item We have four combinations -- SISD, SIMD, MISD, and MIMD.
\end{itemize}
\end{frame}

\subsection{SISD}

\begin{frame}
\frametitle{SISD}
\begin{itemize}
\item SISD (single instruction, single data) 
\item A computer architecture in which a single uniprocessor executes
  a single instruction stream to operate on a data stream.
\item Standard von Neumann architecture.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{SISD}
\centerline{\pgfimage[height=0.6\textheight]{SISD}}
\footnote{\url{http://en.wikipedia.org/wiki/SISD}}
\end{frame}

\begin{frame}
\frametitle{Old School}
\begin{itemize}
\item This is how we have been doing -- sequential programming.
\item The compiler for sequential programming is quite mature and can
  transform source program into efficient binaries.
\item System support for sequential programming, e.g., system call,
  user library, debugging are also very useful.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Give an example of SISD machine.
\end{itemize}
\end{frame}

\subsection{SIMD}

\begin{frame}
\frametitle{SIMD}
\begin{itemize}
\item Single instruction, multiple data (SIMD) 
\item A computer architecture in which multiple processors execute a
  single instruction stream to operate on multiple data streams.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{SIMD}
\centerline{\pgfimage[height=0.6\textheight]{SIMD}}
\footnote{\url{http://en.wikipedia.org/wiki/SIMD}}
\end{frame}

\begin{frame}
\frametitle{SIMD}
\begin{itemize}
\item Strongly related to data parallel programming since the same
  instruction is applied on different data, so as to achieve
  performance by data parallelism.
\item The architecture of GPUs follows the SIMD model -- the host
  issues the same command to all processors so as to process a large
  amount of data simultaneously.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Give an example of SIMD machine.
\end{itemize}
\end{frame}


\subsection{MIMD}

\begin{frame}
\frametitle{MIMD}
\begin{itemize}
\item Multiple instruction, multiple data (MIMD) 
\item A computer architecture in which multiple processors execute
  multiple instruction streams to operate on multiple data streams.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{MIMD}
\centerline{\pgfimage[height=0.6\textheight]{MIMD}}
\footnote{\url{http://en.wikipedia.org/wiki/MIMD}}
\end{frame}

\begin{frame}
\frametitle{MIMD}
\begin{itemize}
\item Strongly related to functional parallelism since different
  processors executes different instructions on different data.
\item The instructions are different because they are from different
  tasks in a wavefront, i.e., tasks that can be done in parallel.
\item The data are different because they are for different tasks.
\item Most multicomputers support MIMD computation model -- computers
  works independently, and synchronize themselves when necessary.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Give an example of MIMD machine.
\end{itemize}
\end{frame}

\subsection{MISD}

\begin{frame}
\huge Wait! You forgot MISD!
\end{frame}

\begin{frame}
\frametitle{MISD}
\begin{itemize}
\item Multiple instruction, single data (MISD) 
\item A computer architecture in which multiple processors execute
  multiple instruction streams to operate on a single data stream.
\item Does it make sense to you?
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{MISD}
\centerline{\pgfimage[height=0.6\textheight]{MISD}}
\footnote{\url{http://en.wikipedia.org/wiki/MISD}}
\end{frame}

\begin{frame}
\frametitle{Fault Tolerance}
\begin{itemize}
\item In fact if we consider MISD as multiple copies of SISD, then it
  can tolerate faults.
\item The same computation is repeated multiple times, by different
  processors, so that at least of them can deliver the results, in
  case some of them fail.
\item For example, in Google MapRedcue computation certain tasks are
  duplicated exactly for the purpose of fault tolerance and
  performance improvement.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Discussion}
\begin{itemize}
\item Give an example of MIMD machine.
\end{itemize}
\end{frame}

\end{CJK}
\end{document}
